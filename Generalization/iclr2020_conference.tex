
\documentclass{article} % For LaTeX2e
\usepackage{iclr2020_conference,times}
\usepackage{graphicx}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{balance}
\usepackage{comment}
\usepackage{enumerate}
\usepackage{amssymb,amsmath}
\usepackage{afterpage}
\usepackage{url}
\usepackage{xspace}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{booktabs}
\usepackage{float}
\let\labelindent\relax
\usepackage{enumitem}
\usepackage{mdwlist}
\usepackage{textcomp}
\usepackage{siunitx}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{bbm}


\setcounter{secnumdepth}{0} %May be changed to 1 or 2 if section numbers are desired.

\allowdisplaybreaks

\newtheorem{theorem}{Theorem}

\newcommand{\figref}[1]{Fig.~\ref{#1}}
\newcommand{\figsubref}[1]{\subref{#1}}
\newcommand{\tabref}[1]{Table~\ref{#1}}
\newcommand{\tabsubref}[1]{Table~\subref{#1}}
\newcommand{\secref}[1]{Sec.~\ref{#1}}
\newcommand{\equref}[1]{Eq.~(\ref{#1})}
\newcommand{\appref}[1]{(App.~\ref{#1})}
\renewcommand{\algref}[1]{Alg.~\ref{#1}}
\newcommand{\theoremref}[1]{Theorem~\ref{#1}}

\newcommand{\etal}{\emph{et~al.}\xspace}
\newcommand{\eg}{\emph{e.g.},\xspace}
\newcommand{\ie}{\emph{i.e.},\xspace}
\newcommand{\etc}{etc.\xspace}
\newcommand{\cf}{\emph{cf.}\xspace}

\newcommand{\highlight}[1]{\textit{#1}}
\newcommand\fakeparagraph[1]{\par\noindent\textbf{{#1}}.\xspace}
\newcommand\fakeparagraphnodot[1]{\par\noindent\textbf{{#1}}\xspace}
\newcommand\circled[1]{\textcircled{\small{#1}}}

\newcommand{\aucroc}{AUC-ROC\xspace}

\usepackage{color}
\newcommand{\olga}[1]{{\color{red} Olga: #1}}
\newcommand{\rahim}[1]{{\color{blue} Rahim: #1}}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}


\title{Generalization}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Rahim Entezari}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
Since September 2019, I was reading about Robustness+ Network Compression. The more I am reading, the more I get that there is a strong correlation between robustness/stability, generalization, generalization bounds, and compression. I found some interesting papers and talks regarding this and I will cover them here.
\end{abstract}

\section{Generalization and compression}
The whole story starts with \figref{generalization_mystery} . While we expect that with more parameters in model we have less generalization, in Deep Neural Network it is not the case.

\begin{figure}[h]
\begin{center}
\includegraphics[width=0.5\columnwidth]{pix/ICML2019Slides.png}
\caption{Generalizaion gets better with overparametrized deep neural networks(~\cite{arora2018stronger})}
\end{center}
\label{generalization_mystery}
\end{figure}

In order to solve this mystery we might have a look on generalization bounds in ML:
\begin{equation}
	error_{test} \leq error_{training} + c \sqrt{\frac{capacity}{m}}
\label{gen_ml}
\end{equation}
In which, the capacity roughly corresponds to the number of parameters and m is the number of training samples.
However \eqref{gen_ml} is not working for DL because the number of params are way more than m(very loose bound). Many other bounds proposed but their common problem is that they are way more than number of params.

\begin{figure}[h]
\begin{center}
\includegraphics[width=0.5\columnwidth]{pix/NormbasedGenBounds.png}
\caption{Norm based generalization bounds are way more than number of parameters (~\cite{arora2018stronger})}
\end{center}
\label{generalization_mystery}
\end{figure}

~\cite{arora2018stronger} tries to answer this question: What property of network trained on real data can help to sharpen this bound? They defined noise stability of trained neural network: How injected gaussian noise at a layer affects higher layers? \ie noise propagation in layers.

















\bibliography{iclr2020_conference}
\bibliographystyle{iclr2020_conference}

\appendix
\section{Appendix}
You may include other additional sections here. 

\end{document}
